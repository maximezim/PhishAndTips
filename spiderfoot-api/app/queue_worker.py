import threading
import time
import uuid
import json
import subprocess
import os

from app.database import SessionLocal
from app.models import SpiderfootScan

# A simple in-memory queue
scan_queue = []

def run_spiderfoot_scan(scan_id: uuid.UUID, target: str, modules: str):
    # Define paths for logs and output
    command_log_file = f"/tmp/{scan_id}_commands.log"
    output_file = f"/tmp/{scan_id}.json"

    # Prepare the commands to send to sfcli.py
    # Start the scan and then exit
    commands = f"start {target} -u all -n {scan_id}\nexit\n"

    # Command to start sfcli.py with server URL and log file
    cmd = [
        "python3",
        "/opt/spiderfoot/sfcli.py",
        "-s", "http://spiderfoot:5001",
        "-o", command_log_file  # Spool commands and output to log file
    ]

    try:
        # Run sfcli.py and send commands via stdin
        process = subprocess.Popen(
            cmd,
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )
        stdout, stderr = process.communicate(commands)

        if process.returncode != 0:
            # Handle errors from the subprocess
            result_data = {
                "error": f"SpiderFoot CLI error: {stderr.strip()}"
            }
        else:
            # Check if the command log file exists
            if os.path.exists(command_log_file):
                with open(command_log_file, 'r') as f:
                    log_data = f.read()
                os.remove(command_log_file)

                # Optionally, parse log_data to confirm scan started
                # For simplicity, we'll assume the scan started successfully
                result_data = {"message": "Scan started successfully."}
            else:
                result_data = {"error": "No command log file generated by SpiderFoot."}

        # Update the database with the scan status as 'running'
        db = SessionLocal()
        scan = db.query(SpiderfootScan).filter(SpiderfootScan.id == scan_id).first()
        if scan:
            scan.status = "running"
            scan.result = result_data
            db.commit()
        db.close()

        # Wait for the scan to complete
        # This is a simplified approach; consider implementing polling for a robust solution
        time.sleep(120)  # Wait 2 minutes; adjust based on expected scan duration

        # Export the scan results as JSON
        export_commands = f"export {scan_id} -t json -f {output_file}\nexit\n"

        export_cmd = [
            "python3",
            "/opt/spiderfoot/sfcli.py",
            "-s", "http://spiderfoot:5001",
            "-o", f"/tmp/{scan_id}_export.log"  # Log export commands and output
        ]

        # Run sfcli.py to export the scan results
        export_process = subprocess.Popen(
            export_cmd,
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )
        export_stdout, export_stderr = export_process.communicate(export_commands)

        if export_process.returncode != 0:
            # Handle export errors
            export_result = {
                "error": f"SpiderFoot export error: {export_stderr.strip()}"
            }
        else:
            # Read the exported JSON file
            if os.path.exists(output_file):
                with open(output_file, 'r') as f:
                    try:
                        export_data = json.load(f)
                    except json.JSONDecodeError:
                        export_data = {"error": "Failed to parse JSON output from SpiderFoot export."}
                os.remove(output_file)
                # Remove the export log file
                os.remove(f"/tmp/{scan_id}_export.log")
                export_result = export_data
            else:
                export_result = {"error": "No output file generated by SpiderFoot export."}

        # Update the database with the scan status and results
        db = SessionLocal()
        scan = db.query(SpiderfootScan).filter(SpiderfootScan.id == scan_id).first()
        if scan:
            scan.status = "completed"
            scan.result = export_result
            db.commit()
        db.close()

    except Exception as e:
        # Handle unexpected exceptions
        db = SessionLocal()
        scan = db.query(SpiderfootScan).filter(SpiderfootScan.id == scan_id).first()
        if scan:
            scan.status = "error"
            scan.result = {"error": str(e)}
            db.commit()
        db.close()

def queue_worker():
    while True:
        if scan_queue:
            db = SessionLocal()
            scan_info = scan_queue.pop(0)
            scan_id, target, modules = scan_info["id"], scan_info["target"], scan_info["modules"]
            
            # Update scan status to 'running'
            scan = db.query(SpiderfootScan).filter(SpiderfootScan.id == scan_id).first()
            if scan:
                scan.status = "running"
                db.commit()
            db.close()
            
            # Execute the scan
            run_spiderfoot_scan(scan_id, target, modules)
        else:
            time.sleep(2)

# Start the worker thread
worker_thread = threading.Thread(target=queue_worker, daemon=True)
worker_thread.start()
