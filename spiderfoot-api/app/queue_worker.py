import threading
import time
import uuid
import json
import subprocess
import os
import re

from app.database import SessionLocal
from app.models import SpiderfootScan

# A simple in-memory queue
scan_queue = []

def run_spiderfoot_scan(scan_id: uuid.UUID, target: str, modules: str):
    # Define paths for logs and output
    command_log_file = f"/tmp/{scan_id}_commands.log"
    output_file = f"/tmp/{scan_id}.json"

    # Prepare the commands to send to sfcli.py
    # The '-n {scan_id}' sets the name of the scan, not its ID. SpiderFoot will assign an actual scan ID.
    commands = f"start {target} -u all -n {scan_id}\nexit\n"

    # Command to start sfcli.py with server URL and log file
    cmd = [
        "python3",
        "/opt/spiderfoot/sfcli.py",
        "-s", "http://spiderfoot:5001",
        "-o", command_log_file
    ]

    try:
        # Run sfcli.py and send commands via stdin
        process = subprocess.Popen(
            cmd,
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )
        stdout, stderr = process.communicate(commands)

        db = SessionLocal()

        # Check for errors
        if process.returncode != 0:
            # Handle errors from the subprocess
            result_data = {
                "error": f"SpiderFoot CLI error: {stderr.strip()}"
            }

            # Update the database with the scan status as 'error'
            scan = db.query(SpiderfootScan).filter(SpiderfootScan.id == scan_id).first()
            if scan:
                scan.status = "error"
                scan.result = result_data
                db.commit()
            db.close()
            return
        else:
            # Check if the command log file exists
            if os.path.exists(command_log_file):
                with open(command_log_file, 'r') as f:
                    log_data = f.read()
                os.remove(command_log_file)

                # Extract the actual SpiderFoot scan ID from the log
                # The log line typically looks like: [*] Scan ID: F25409E4
                match = re.search(r"\[\*\]\s*Scan ID:\s*(\S+)", log_data)
                if match:
                    spiderfoot_id = match.group(1).strip()
                    result_data = {
                        "message": "Scan started successfully.",
                        "spiderfoot_id": spiderfoot_id
                    }
                else:
                    spiderfoot_id = None
                    result_data = {
                        "message": "Scan started successfully, but could not parse SpiderFoot scan ID."
                    }
            else:
                spiderfoot_id = None
                result_data = {"error": "No command log file generated by SpiderFoot."}

        # Update the database with the scan status as 'running' and store the SpiderFoot scan ID
        scan = db.query(SpiderfootScan).filter(SpiderfootScan.id == scan_id).first()
        if scan:
            scan.status = "running"
            scan.result = result_data
            if spiderfoot_id:
                # Assuming your SpiderfootScan model has a column named `external_id` to store this
                scan.external_id = spiderfoot_id
            db.commit()
        db.close()

        # If we don't have a SpiderFoot scan ID, we cannot proceed with exporting results.
        if not spiderfoot_id:
            return

        # Wait for the scan to complete
        # This is a simplified approach; consider implementing polling for a robust solution
        time.sleep(120)  # Wait 2 minutes; adjust based on expected scan duration

        # Export the scan results as JSON using the SpiderFoot assigned ID
        export_commands = f"export {spiderfoot_id} -t json -f {output_file}\nexit\n"

        export_cmd = [
            "python3",
            "/opt/spiderfoot/sfcli.py",
            "-s", "http://spiderfoot:5001",
            "-o", f"/tmp/{scan_id}_export.log"
        ]

        # Run sfcli.py to export the scan results
        export_process = subprocess.Popen(
            export_cmd,
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )
        export_stdout, export_stderr = export_process.communicate(export_commands)

        if export_process.returncode != 0:
            # Handle export errors
            export_result = {
                "error": f"SpiderFoot export error: {export_stderr.strip()}"
            }
        else:
            # Read the exported JSON file
            if os.path.exists(output_file):
                with open(output_file, 'r') as f:
                    try:
                        export_data = json.load(f)
                    except json.JSONDecodeError:
                        export_data = {"error": "Failed to parse JSON output from SpiderFoot export."}
                os.remove(output_file)
                # Remove the export log file
                if os.path.exists(f"/tmp/{scan_id}_export.log"):
                    os.remove(f"/tmp/{scan_id}_export.log")
                export_result = export_data
            else:
                export_result = {"error": "No output file generated by SpiderFoot export."}

        # Update the database with the scan status and results
        db = SessionLocal()
        scan = db.query(SpiderfootScan).filter(SpiderfootScan.id == scan_id).first()
        if scan:
            scan.status = "completed"
            scan.result = export_result
            db.commit()
        db.close()

    except Exception as e:
        db = SessionLocal()
        scan = db.query(SpiderfootScan).filter(SpiderfootScan.id == scan_id).first()
        if scan:
            scan.status = "error"
            scan.result = {"error": str(e)}
            db.commit()
        db.close()

def queue_worker():
    while True:
        if scan_queue:
            db = SessionLocal()
            scan_info = scan_queue.pop(0)
            scan_id, target, modules = scan_info["id"], scan_info["target"], scan_info["modules"]
            
            # Update scan status to 'running'
            scan = db.query(SpiderfootScan).filter(SpiderfootScan.id == scan_id).first()
            if scan:
                scan.status = "running"
                db.commit()
            db.close()
            
            # Execute the scan
            run_spiderfoot_scan(scan_id, target, modules)
        else:
            time.sleep(2)

# Start the worker thread
worker_thread = threading.Thread(target=queue_worker, daemon=True)
worker_thread.start()
